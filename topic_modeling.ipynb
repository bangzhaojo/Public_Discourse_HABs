{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11d36a13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# from pandas.plotting import register_matplotlib_converters\n",
    "# register_matplotlib_converters()\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c518ec3-76f2-40cd-8096-2dc5efd7ab8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6d73fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elain\\anaconda\\envs\\bertopic\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1b483a3f-2c3e-4bda-818d-6fe3cda42da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_RC_with_labels_addition.json', 'r') as json_file:\n",
    "    json_objects = json.load(json_file)\n",
    "    \n",
    "RCs = []\n",
    "\n",
    "for comment in json_objects:\n",
    "    if comment['label']==1:\n",
    "        RCs.append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b72a944b-8b7b-4aa9-bc8b-b8a40193b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model = BERTopic.load(\"davanstrien/chat_topics\")\n",
    "\n",
    "# example = RCs[1]['body']\n",
    "# topic, prob = topic_model.transform(example)\n",
    "# topics = topic_model.get_topic_info()\n",
    "\n",
    "# topics[topics['Topic']==topic[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d3fc0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [item['body'] for item in RCs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1c198a6d-f49a-4144-a89e-4a7b389ef1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That is the thing, I understand these lines are just drawn somewhere and some states/cities can culturally identify with a different area, but the only \"coast\" Ohio has is Lake Erie.',\n",
       " 'Hmm I guess we all have different views. I work for a Planning District Commission (urban and regional city planning) and this is not our general consensus about the James alongside our environmental specialists. Still regarded as having high TMDL levels (total maximum daily load of toxins in a water body) and sediment levels. Just my experience and research. Probably safer than 10 years ago, but not safe enough to swim in without moderate health risk. Personally don’t let me dog go near the water, just the rocks',\n",
       " \"Lake Erie, it's just gross.\",\n",
       " 'I agree that algae is quite normal and this does happen in the wild.  However, I believe fertilizers can exacerbate this….  Similar to lake Eries algae bloom',\n",
       " 'Ohio has a water border with Canada in Lake Erie']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "907dbed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\"english\")\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8bd73108-593b-4ce4-bb7c-f6bc2d641e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(embedding_model=\"sentence-t5-base\", min_topic_size=100, nr_topics=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "63f868ff-8f5e-4587-a0c5-a6d7572af9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=10, n_components=5, \n",
    "                  min_dist=0.0, metric='cosine', random_state=42)\n",
    "topic_model = BERTopic(umap_model=umap_model, embedding_model=\"sentence-t5-base\", min_topic_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "55e5e435-0323-4566-8c74-4d10c96fc3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e71e782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2ace3c6a-410e-4670-b6be-2fcf053b8f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>126</td>\n",
       "      <td>-1_the_and_when_of</td>\n",
       "      <td>[the, and, when, of, they, came, november, in,...</td>\n",
       "      <td>[The legend lives on from the Chippewa on down...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5324</td>\n",
       "      <td>0_the_and_to_of</td>\n",
       "      <td>[the, and, to, of, lake, in, erie, it, is, that]</td>\n",
       "      <td>[&amp;gt; When you say \"Lake Accotink Dam has sign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>1_balloons_the_of_event</td>\n",
       "      <td>[balloons, the, of, event, in, and, guard, sea...</td>\n",
       "      <td>[“Typically, a helium-filled latex balloon tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>2_erie_lake_is_that</td>\n",
       "      <td>[erie, lake, is, that, in, on, coast, up, of, ...</td>\n",
       "      <td>[Lake erie?, lake erie, Lake erie]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                     Name  \\\n",
       "0     -1    126       -1_the_and_when_of   \n",
       "1      0   5324          0_the_and_to_of   \n",
       "2      1    105  1_balloons_the_of_event   \n",
       "3      2     55      2_erie_lake_is_that   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [the, and, when, of, they, came, november, in,...   \n",
       "1   [the, and, to, of, lake, in, erie, it, is, that]   \n",
       "2  [balloons, the, of, event, in, and, guard, sea...   \n",
       "3  [erie, lake, is, that, in, on, coast, up, of, ...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [The legend lives on from the Chippewa on down...  \n",
       "1  [&gt; When you say \"Lake Accotink Dam has sign...  \n",
       "2  [“Typically, a helium-filled latex balloon tha...  \n",
       "3                 [Lake erie?, lake erie, Lake erie]  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c022f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list.to_csv('topic_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cdc379f7-94bd-4902-a11e-0c814ddf9881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.10346325880666568),\n",
       " ('and', 0.06865365708447345),\n",
       " ('to', 0.06674831598875453),\n",
       " ('of', 0.0649894119038331),\n",
       " ('lake', 0.061778493203813886),\n",
       " ('in', 0.0575515014777078),\n",
       " ('erie', 0.05331504873703368),\n",
       " ('it', 0.047311525987262154),\n",
       " ('is', 0.045339294945078414),\n",
       " ('that', 0.04388214102126034)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5e4e180e-b457-42cc-9aa5-c618c88fc40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Representative_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That is the thing, I understand these lines ar...</td>\n",
       "      <td>1</td>\n",
       "      <td>1_the_and_to_of</td>\n",
       "      <td>[the, and, to, of, lake, in, erie, it, is, that]</td>\n",
       "      <td>[Yup I believe that is the case. It was a meta...</td>\n",
       "      <td>the - and - to - of - lake - in - erie - it - ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hmm I guess we all have different views. I wor...</td>\n",
       "      <td>1</td>\n",
       "      <td>1_the_and_to_of</td>\n",
       "      <td>[the, and, to, of, lake, in, erie, it, is, that]</td>\n",
       "      <td>[Yup I believe that is the case. It was a meta...</td>\n",
       "      <td>the - and - to - of - lake - in - erie - it - ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lake Erie, it's just gross.</td>\n",
       "      <td>1</td>\n",
       "      <td>1_the_and_to_of</td>\n",
       "      <td>[the, and, to, of, lake, in, erie, it, is, that]</td>\n",
       "      <td>[Yup I believe that is the case. It was a meta...</td>\n",
       "      <td>the - and - to - of - lake - in - erie - it - ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I agree that algae is quite normal and this do...</td>\n",
       "      <td>1</td>\n",
       "      <td>1_the_and_to_of</td>\n",
       "      <td>[the, and, to, of, lake, in, erie, it, is, that]</td>\n",
       "      <td>[Yup I believe that is the case. It was a meta...</td>\n",
       "      <td>the - and - to - of - lake - in - erie - it - ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ohio has a water border with Canada in Lake Erie</td>\n",
       "      <td>1</td>\n",
       "      <td>1_the_and_to_of</td>\n",
       "      <td>[the, and, to, of, lake, in, erie, it, is, that]</td>\n",
       "      <td>[Yup I believe that is the case. It was a meta...</td>\n",
       "      <td>the - and - to - of - lake - in - erie - it - ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5605</th>\n",
       "      <td>Yet more recent - like half of Lake Erie was c...</td>\n",
       "      <td>1</td>\n",
       "      <td>1_the_and_to_of</td>\n",
       "      <td>[the, and, to, of, lake, in, erie, it, is, that]</td>\n",
       "      <td>[Yup I believe that is the case. It was a meta...</td>\n",
       "      <td>the - and - to - of - lake - in - erie - it - ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5606</th>\n",
       "      <td>The water was too poisonous for any life. The ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1_the_and_to_of</td>\n",
       "      <td>[the, and, to, of, lake, in, erie, it, is, that]</td>\n",
       "      <td>[Yup I believe that is the case. It was a meta...</td>\n",
       "      <td>the - and - to - of - lake - in - erie - it - ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5607</th>\n",
       "      <td>Im not saying theyre perfect, there is a lot o...</td>\n",
       "      <td>1</td>\n",
       "      <td>1_the_and_to_of</td>\n",
       "      <td>[the, and, to, of, lake, in, erie, it, is, that]</td>\n",
       "      <td>[Yup I believe that is the case. It was a meta...</td>\n",
       "      <td>the - and - to - of - lake - in - erie - it - ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5608</th>\n",
       "      <td>Fair point. The headline and the article itsel...</td>\n",
       "      <td>1</td>\n",
       "      <td>1_the_and_to_of</td>\n",
       "      <td>[the, and, to, of, lake, in, erie, it, is, that]</td>\n",
       "      <td>[Yup I believe that is the case. It was a meta...</td>\n",
       "      <td>the - and - to - of - lake - in - erie - it - ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5609</th>\n",
       "      <td>Not for long when the effects of the Trump EPA...</td>\n",
       "      <td>1</td>\n",
       "      <td>1_the_and_to_of</td>\n",
       "      <td>[the, and, to, of, lake, in, erie, it, is, that]</td>\n",
       "      <td>[Yup I believe that is the case. It was a meta...</td>\n",
       "      <td>the - and - to - of - lake - in - erie - it - ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5610 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Document  Topic  \\\n",
       "0     That is the thing, I understand these lines ar...      1   \n",
       "1     Hmm I guess we all have different views. I wor...      1   \n",
       "2                           Lake Erie, it's just gross.      1   \n",
       "3     I agree that algae is quite normal and this do...      1   \n",
       "4      Ohio has a water border with Canada in Lake Erie      1   \n",
       "...                                                 ...    ...   \n",
       "5605  Yet more recent - like half of Lake Erie was c...      1   \n",
       "5606  The water was too poisonous for any life. The ...      1   \n",
       "5607  Im not saying theyre perfect, there is a lot o...      1   \n",
       "5608  Fair point. The headline and the article itsel...      1   \n",
       "5609  Not for long when the effects of the Trump EPA...      1   \n",
       "\n",
       "                 Name                                    Representation  \\\n",
       "0     1_the_and_to_of  [the, and, to, of, lake, in, erie, it, is, that]   \n",
       "1     1_the_and_to_of  [the, and, to, of, lake, in, erie, it, is, that]   \n",
       "2     1_the_and_to_of  [the, and, to, of, lake, in, erie, it, is, that]   \n",
       "3     1_the_and_to_of  [the, and, to, of, lake, in, erie, it, is, that]   \n",
       "4     1_the_and_to_of  [the, and, to, of, lake, in, erie, it, is, that]   \n",
       "...               ...                                               ...   \n",
       "5605  1_the_and_to_of  [the, and, to, of, lake, in, erie, it, is, that]   \n",
       "5606  1_the_and_to_of  [the, and, to, of, lake, in, erie, it, is, that]   \n",
       "5607  1_the_and_to_of  [the, and, to, of, lake, in, erie, it, is, that]   \n",
       "5608  1_the_and_to_of  [the, and, to, of, lake, in, erie, it, is, that]   \n",
       "5609  1_the_and_to_of  [the, and, to, of, lake, in, erie, it, is, that]   \n",
       "\n",
       "                                    Representative_Docs  \\\n",
       "0     [Yup I believe that is the case. It was a meta...   \n",
       "1     [Yup I believe that is the case. It was a meta...   \n",
       "2     [Yup I believe that is the case. It was a meta...   \n",
       "3     [Yup I believe that is the case. It was a meta...   \n",
       "4     [Yup I believe that is the case. It was a meta...   \n",
       "...                                                 ...   \n",
       "5605  [Yup I believe that is the case. It was a meta...   \n",
       "5606  [Yup I believe that is the case. It was a meta...   \n",
       "5607  [Yup I believe that is the case. It was a meta...   \n",
       "5608  [Yup I believe that is the case. It was a meta...   \n",
       "5609  [Yup I believe that is the case. It was a meta...   \n",
       "\n",
       "                                            Top_n_words  Probability  \\\n",
       "0     the - and - to - of - lake - in - erie - it - ...          1.0   \n",
       "1     the - and - to - of - lake - in - erie - it - ...          1.0   \n",
       "2     the - and - to - of - lake - in - erie - it - ...          1.0   \n",
       "3     the - and - to - of - lake - in - erie - it - ...          1.0   \n",
       "4     the - and - to - of - lake - in - erie - it - ...          1.0   \n",
       "...                                                 ...          ...   \n",
       "5605  the - and - to - of - lake - in - erie - it - ...          1.0   \n",
       "5606  the - and - to - of - lake - in - erie - it - ...          1.0   \n",
       "5607  the - and - to - of - lake - in - erie - it - ...          1.0   \n",
       "5608  the - and - to - of - lake - in - erie - it - ...          1.0   \n",
       "5609  the - and - to - of - lake - in - erie - it - ...          1.0   \n",
       "\n",
       "      Representative_document  \n",
       "0                       False  \n",
       "1                       False  \n",
       "2                       False  \n",
       "3                       False  \n",
       "4                       False  \n",
       "...                       ...  \n",
       "5605                    False  \n",
       "5606                    False  \n",
       "5607                    False  \n",
       "5608                    False  \n",
       "5609                    False  \n",
       "\n",
       "[5610 rows x 8 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_document_info(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "96f50cdf-a717-4366-afaf-7b5cd8e1342d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable BERTopic object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[182], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m new_topics, new_probs \u001b[38;5;241m=\u001b[39m topic_model\u001b[38;5;241m.\u001b[39mreduce_topics(docs\u001b[38;5;241m=\u001b[39mtexts, nr_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable BERTopic object"
     ]
    }
   ],
   "source": [
    "new_topics, new_probs = topic_model.reduce_topics(docs=texts, nr_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18cf82e-b8aa-4a8d-a977-087acdfc1e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: nfl 1： Cleveland 2: URL, 3: Fish 4: water quality/nutrient  5: pollution 6:cold winter 7: hue jackson, football 8: cold winter 9: browns, football 10:geographic stuff?\n",
    "# 11: boat  12: fishery 13: lake erie monsters, ice hockey 14: beach, glass, rocks 15: just mention lake erie 16 swim 17 a question about lake erie 18: buffalo and giagara\n",
    "# 19: bot and url 20: beach 21: fire 22: talks about their families 23: lake erie bros, nfl 24: water elevation 25: location of lake erie 26: something like 'lake erie'.\n",
    "# 27: github links (tmdl) 28: urls 29: british battle 1812 30: boat sailing 31: wind, power, nuclear, energy, solar 32: Cedar Point, coaster 33: mayflies, they, bugs\n",
    "# 34: Cedar Point Trip Report 35: birds 36: sky and ufo 37: lake erie is beautiful38: football 39: buffalo 40: district, trump, republican, county, ohio\n",
    "# 41: balloons, event, balloonfest 42: 'lake erie' 43: railroad, rail, train 44: war, british, battle, american 45: never seen lake erie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1f0fc-7ebe-43a5-a4ac-020d441b255e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fae52f5c-d11b-4b1b-b672-175fed40a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_document_info(texts)[topic_model.get_document_info(texts)['Topic']==6][['Document']].to_csv('topic6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "92ebbf8c-e608-4a72-b987-e63891254adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Representative_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>Apparently she lived to be 88.\\n\\nhttps://en.w...</td>\n",
       "      <td>58</td>\n",
       "      <td>58_he_her_was_she</td>\n",
       "      <td>[he, her, was, she, his, case, the, found, mis...</td>\n",
       "      <td>[Firstly I'd like to commend James Renner on t...</td>\n",
       "      <td>he - her - was - she - his - case - the - foun...</td>\n",
       "      <td>0.762799</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>There was an episode of Criminal Minds that to...</td>\n",
       "      <td>58</td>\n",
       "      <td>58_he_her_was_she</td>\n",
       "      <td>[he, her, was, she, his, case, the, found, mis...</td>\n",
       "      <td>[Firstly I'd like to commend James Renner on t...</td>\n",
       "      <td>he - her - was - she - his - case - the - foun...</td>\n",
       "      <td>0.920053</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>Starts from St Lawrence River in East Quebec a...</td>\n",
       "      <td>58</td>\n",
       "      <td>58_he_her_was_she</td>\n",
       "      <td>[he, her, was, she, his, case, the, found, mis...</td>\n",
       "      <td>[Firstly I'd like to commend James Renner on t...</td>\n",
       "      <td>he - her - was - she - his - case - the - foun...</td>\n",
       "      <td>0.942156</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>Church St. poltergeist (now a pizza place), Bl...</td>\n",
       "      <td>58</td>\n",
       "      <td>58_he_her_was_she</td>\n",
       "      <td>[he, her, was, she, his, case, the, found, mis...</td>\n",
       "      <td>[Firstly I'd like to commend James Renner on t...</td>\n",
       "      <td>he - her - was - she - his - case - the - foun...</td>\n",
       "      <td>0.776802</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>This is a burner account because I don't want ...</td>\n",
       "      <td>58</td>\n",
       "      <td>58_he_her_was_she</td>\n",
       "      <td>[he, her, was, she, his, case, the, found, mis...</td>\n",
       "      <td>[Firstly I'd like to commend James Renner on t...</td>\n",
       "      <td>he - her - was - she - his - case - the - foun...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48308</th>\n",
       "      <td>.........\\n\\n\"Your honor, I do solomly swear t...</td>\n",
       "      <td>58</td>\n",
       "      <td>58_he_her_was_she</td>\n",
       "      <td>[he, her, was, she, his, case, the, found, mis...</td>\n",
       "      <td>[Firstly I'd like to commend James Renner on t...</td>\n",
       "      <td>he - her - was - she - his - case - the - foun...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48937</th>\n",
       "      <td>Natasha Ryan, she was a 14-year-old girl who w...</td>\n",
       "      <td>58</td>\n",
       "      <td>58_he_her_was_she</td>\n",
       "      <td>[he, her, was, she, his, case, the, found, mis...</td>\n",
       "      <td>[Firstly I'd like to commend James Renner on t...</td>\n",
       "      <td>he - her - was - she - his - case - the - foun...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48944</th>\n",
       "      <td>What about this?\\n\\nhttps://www.clevescene.com...</td>\n",
       "      <td>58</td>\n",
       "      <td>58_he_her_was_she</td>\n",
       "      <td>[he, her, was, she, his, case, the, found, mis...</td>\n",
       "      <td>[Firstly I'd like to commend James Renner on t...</td>\n",
       "      <td>he - her - was - she - his - case - the - foun...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49039</th>\n",
       "      <td>She was last seen in Toronto, her shoes and ot...</td>\n",
       "      <td>58</td>\n",
       "      <td>58_he_her_was_she</td>\n",
       "      <td>[he, her, was, she, his, case, the, found, mis...</td>\n",
       "      <td>[Firstly I'd like to commend James Renner on t...</td>\n",
       "      <td>he - her - was - she - his - case - the - foun...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50184</th>\n",
       "      <td>[Lawrence Bader](https://en.m.wikipedia.org/wi...</td>\n",
       "      <td>58</td>\n",
       "      <td>58_he_her_was_she</td>\n",
       "      <td>[he, her, was, she, his, case, the, found, mis...</td>\n",
       "      <td>[Firstly I'd like to commend James Renner on t...</td>\n",
       "      <td>he - her - was - she - his - case - the - foun...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Document  Topic  \\\n",
       "437    Apparently she lived to be 88.\\n\\nhttps://en.w...     58   \n",
       "847    There was an episode of Criminal Minds that to...     58   \n",
       "1220   Starts from St Lawrence River in East Quebec a...     58   \n",
       "2557   Church St. poltergeist (now a pizza place), Bl...     58   \n",
       "2570   This is a burner account because I don't want ...     58   \n",
       "...                                                  ...    ...   \n",
       "48308  .........\\n\\n\"Your honor, I do solomly swear t...     58   \n",
       "48937  Natasha Ryan, she was a 14-year-old girl who w...     58   \n",
       "48944  What about this?\\n\\nhttps://www.clevescene.com...     58   \n",
       "49039  She was last seen in Toronto, her shoes and ot...     58   \n",
       "50184  [Lawrence Bader](https://en.m.wikipedia.org/wi...     58   \n",
       "\n",
       "                    Name                                     Representation  \\\n",
       "437    58_he_her_was_she  [he, her, was, she, his, case, the, found, mis...   \n",
       "847    58_he_her_was_she  [he, her, was, she, his, case, the, found, mis...   \n",
       "1220   58_he_her_was_she  [he, her, was, she, his, case, the, found, mis...   \n",
       "2557   58_he_her_was_she  [he, her, was, she, his, case, the, found, mis...   \n",
       "2570   58_he_her_was_she  [he, her, was, she, his, case, the, found, mis...   \n",
       "...                  ...                                                ...   \n",
       "48308  58_he_her_was_she  [he, her, was, she, his, case, the, found, mis...   \n",
       "48937  58_he_her_was_she  [he, her, was, she, his, case, the, found, mis...   \n",
       "48944  58_he_her_was_she  [he, her, was, she, his, case, the, found, mis...   \n",
       "49039  58_he_her_was_she  [he, her, was, she, his, case, the, found, mis...   \n",
       "50184  58_he_her_was_she  [he, her, was, she, his, case, the, found, mis...   \n",
       "\n",
       "                                     Representative_Docs  \\\n",
       "437    [Firstly I'd like to commend James Renner on t...   \n",
       "847    [Firstly I'd like to commend James Renner on t...   \n",
       "1220   [Firstly I'd like to commend James Renner on t...   \n",
       "2557   [Firstly I'd like to commend James Renner on t...   \n",
       "2570   [Firstly I'd like to commend James Renner on t...   \n",
       "...                                                  ...   \n",
       "48308  [Firstly I'd like to commend James Renner on t...   \n",
       "48937  [Firstly I'd like to commend James Renner on t...   \n",
       "48944  [Firstly I'd like to commend James Renner on t...   \n",
       "49039  [Firstly I'd like to commend James Renner on t...   \n",
       "50184  [Firstly I'd like to commend James Renner on t...   \n",
       "\n",
       "                                             Top_n_words  Probability  \\\n",
       "437    he - her - was - she - his - case - the - foun...     0.762799   \n",
       "847    he - her - was - she - his - case - the - foun...     0.920053   \n",
       "1220   he - her - was - she - his - case - the - foun...     0.942156   \n",
       "2557   he - her - was - she - his - case - the - foun...     0.776802   \n",
       "2570   he - her - was - she - his - case - the - foun...     1.000000   \n",
       "...                                                  ...          ...   \n",
       "48308  he - her - was - she - his - case - the - foun...     1.000000   \n",
       "48937  he - her - was - she - his - case - the - foun...     1.000000   \n",
       "48944  he - her - was - she - his - case - the - foun...     1.000000   \n",
       "49039  he - her - was - she - his - case - the - foun...     1.000000   \n",
       "50184  he - her - was - she - his - case - the - foun...     1.000000   \n",
       "\n",
       "       Representative_document  \n",
       "437                      False  \n",
       "847                      False  \n",
       "1220                     False  \n",
       "2557                     False  \n",
       "2570                     False  \n",
       "...                        ...  \n",
       "48308                    False  \n",
       "48937                    False  \n",
       "48944                    False  \n",
       "49039                    False  \n",
       "50184                    False  \n",
       "\n",
       "[134 rows x 8 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_document_info(texts)[topic_model.get_document_info(texts)['Topic']==58]# .Document.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e64fba67-b089-4344-b464-455fada7d568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Representative_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love your posts! Im on Lake Erie, i wish I c...</td>\n",
       "      <td>14</td>\n",
       "      <td>14_glass_found_beach_find</td>\n",
       "      <td>[glass, found, beach, find, rocks, erie, it, l...</td>\n",
       "      <td>[I am on the west coast, but I can tell you th...</td>\n",
       "      <td>glass - found - beach - find - rocks - erie - ...</td>\n",
       "      <td>0.692482</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is your cat 12?y cat lived until 20.Shwe was o...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_to_lake_and</td>\n",
       "      <td>[the, to, lake, and, of, in, erie, is, it, that]</td>\n",
       "      <td>[Always , always , always tip my take out pers...</td>\n",
       "      <td>the - to - lake - and - of - in - erie - is - ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adding to this if you go a bit farther north p...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_to_lake_and</td>\n",
       "      <td>[the, to, lake, and, of, in, erie, is, it, that]</td>\n",
       "      <td>[Always , always , always tip my take out pers...</td>\n",
       "      <td>the - to - lake - and - of - in - erie - is - ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The UP has black bear, mountain lions, moose, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_to_lake_and</td>\n",
       "      <td>[the, to, lake, and, of, in, erie, is, it, that]</td>\n",
       "      <td>[Always , always , always tip my take out pers...</td>\n",
       "      <td>the - to - lake - and - of - in - erie - is - ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As clear as Lake Erie spring water.  lol</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_to_lake_and</td>\n",
       "      <td>[the, to, lake, and, of, in, erie, is, it, that]</td>\n",
       "      <td>[Always , always , always tip my take out pers...</td>\n",
       "      <td>the - to - lake - and - of - in - erie - is - ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50652</th>\n",
       "      <td>Fair point. The headline and the article itsel...</td>\n",
       "      <td>4</td>\n",
       "      <td>4_water_the_to_that</td>\n",
       "      <td>[water, the, to, that, of, and, in, is, it, are]</td>\n",
       "      <td>[Now for your second question, I am going to q...</td>\n",
       "      <td>water - the - to - that - of - and - in - is -...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50653</th>\n",
       "      <td>Jesus Christ, why did I read this.\\n\\nSomeone ...</td>\n",
       "      <td>7</td>\n",
       "      <td>7_erie_lake_into_hue</td>\n",
       "      <td>[erie, lake, into, hue, he, him, throw, jackso...</td>\n",
       "      <td>[Hue Jackson cursed the water when he jumped i...</td>\n",
       "      <td>erie - lake - into - hue - he - him - throw - ...</td>\n",
       "      <td>0.915887</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50654</th>\n",
       "      <td>**2018 Wild and Scenic Film Festival**\\n\\n[The...</td>\n",
       "      <td>1</td>\n",
       "      <td>1_cleveland_and_you_is</td>\n",
       "      <td>[cleveland, and, you, is, ohio, the, in, of, t...</td>\n",
       "      <td>[Honestly you could spend weeks checking every...</td>\n",
       "      <td>cleveland - and - you - is - ohio - the - in -...</td>\n",
       "      <td>0.890593</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50655</th>\n",
       "      <td>Not for long when the effects of the Trump EPA...</td>\n",
       "      <td>4</td>\n",
       "      <td>4_water_the_to_that</td>\n",
       "      <td>[water, the, to, that, of, and, in, is, it, are]</td>\n",
       "      <td>[Now for your second question, I am going to q...</td>\n",
       "      <td>water - the - to - that - of - and - in - is -...</td>\n",
       "      <td>0.913918</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50656</th>\n",
       "      <td>&amp;gt;Is there a literal dog pound in the stadiu...</td>\n",
       "      <td>9</td>\n",
       "      <td>9_he_team_browns_game</td>\n",
       "      <td>[he, team, browns, game, the, his, to, and, th...</td>\n",
       "      <td>[Fresh pasta\\n\\nIf this is the end of LeBron i...</td>\n",
       "      <td>he - team - browns - game - the - his - to - a...</td>\n",
       "      <td>0.409696</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50657 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Document  Topic  \\\n",
       "0      I love your posts! Im on Lake Erie, i wish I c...     14   \n",
       "1      Is your cat 12?y cat lived until 20.Shwe was o...     -1   \n",
       "2      Adding to this if you go a bit farther north p...     -1   \n",
       "3      The UP has black bear, mountain lions, moose, ...     -1   \n",
       "4               As clear as Lake Erie spring water.  lol     -1   \n",
       "...                                                  ...    ...   \n",
       "50652  Fair point. The headline and the article itsel...      4   \n",
       "50653  Jesus Christ, why did I read this.\\n\\nSomeone ...      7   \n",
       "50654  **2018 Wild and Scenic Film Festival**\\n\\n[The...      1   \n",
       "50655  Not for long when the effects of the Trump EPA...      4   \n",
       "50656  &gt;Is there a literal dog pound in the stadiu...      9   \n",
       "\n",
       "                            Name  \\\n",
       "0      14_glass_found_beach_find   \n",
       "1             -1_the_to_lake_and   \n",
       "2             -1_the_to_lake_and   \n",
       "3             -1_the_to_lake_and   \n",
       "4             -1_the_to_lake_and   \n",
       "...                          ...   \n",
       "50652        4_water_the_to_that   \n",
       "50653       7_erie_lake_into_hue   \n",
       "50654     1_cleveland_and_you_is   \n",
       "50655        4_water_the_to_that   \n",
       "50656      9_he_team_browns_game   \n",
       "\n",
       "                                          Representation  \\\n",
       "0      [glass, found, beach, find, rocks, erie, it, l...   \n",
       "1       [the, to, lake, and, of, in, erie, is, it, that]   \n",
       "2       [the, to, lake, and, of, in, erie, is, it, that]   \n",
       "3       [the, to, lake, and, of, in, erie, is, it, that]   \n",
       "4       [the, to, lake, and, of, in, erie, is, it, that]   \n",
       "...                                                  ...   \n",
       "50652   [water, the, to, that, of, and, in, is, it, are]   \n",
       "50653  [erie, lake, into, hue, he, him, throw, jackso...   \n",
       "50654  [cleveland, and, you, is, ohio, the, in, of, t...   \n",
       "50655   [water, the, to, that, of, and, in, is, it, are]   \n",
       "50656  [he, team, browns, game, the, his, to, and, th...   \n",
       "\n",
       "                                     Representative_Docs  \\\n",
       "0      [I am on the west coast, but I can tell you th...   \n",
       "1      [Always , always , always tip my take out pers...   \n",
       "2      [Always , always , always tip my take out pers...   \n",
       "3      [Always , always , always tip my take out pers...   \n",
       "4      [Always , always , always tip my take out pers...   \n",
       "...                                                  ...   \n",
       "50652  [Now for your second question, I am going to q...   \n",
       "50653  [Hue Jackson cursed the water when he jumped i...   \n",
       "50654  [Honestly you could spend weeks checking every...   \n",
       "50655  [Now for your second question, I am going to q...   \n",
       "50656  [Fresh pasta\\n\\nIf this is the end of LeBron i...   \n",
       "\n",
       "                                             Top_n_words  Probability  \\\n",
       "0      glass - found - beach - find - rocks - erie - ...     0.692482   \n",
       "1      the - to - lake - and - of - in - erie - is - ...     0.000000   \n",
       "2      the - to - lake - and - of - in - erie - is - ...     0.000000   \n",
       "3      the - to - lake - and - of - in - erie - is - ...     0.000000   \n",
       "4      the - to - lake - and - of - in - erie - is - ...     0.000000   \n",
       "...                                                  ...          ...   \n",
       "50652  water - the - to - that - of - and - in - is -...     1.000000   \n",
       "50653  erie - lake - into - hue - he - him - throw - ...     0.915887   \n",
       "50654  cleveland - and - you - is - ohio - the - in -...     0.890593   \n",
       "50655  water - the - to - that - of - and - in - is -...     0.913918   \n",
       "50656  he - team - browns - game - the - his - to - a...     0.409696   \n",
       "\n",
       "       Representative_document  \n",
       "0                        False  \n",
       "1                        False  \n",
       "2                        False  \n",
       "3                        False  \n",
       "4                        False  \n",
       "...                        ...  \n",
       "50652                    False  \n",
       "50653                    False  \n",
       "50654                    False  \n",
       "50655                    False  \n",
       "50656                    False  \n",
       "\n",
       "[50657 rows x 8 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_document_info(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33181c8a-cfb6-44a2-8503-7d167b9f4003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['14_glass_found_beach_find', '-1_the_to_lake_and',\n",
       "       '4_water_the_to_that', '5_lake_erie_water_is',\n",
       "       '0_bros_browns_bills_erie', '11_boat_my_was_to',\n",
       "       '3_walleye_fish_fishing_erie', '20_beach_beaches_erie_lake',\n",
       "       '7_erie_lake_into_hue', '17_erie_lake_ohio_where',\n",
       "       '2_https_com_amp_www', '35_birds_bird_birding_bald',\n",
       "       '12_fish_fishing_walleye_trout', '10_erie_lake_not_no',\n",
       "       '1_cleveland_and_you_is', '8_snow_winter_effect_it',\n",
       "       '15_erie_lake_is_it', '47_park_hocking_hills_erie',\n",
       "       '55_bro_bros_erie_lake', '9_he_team_browns_game',\n",
       "       '45_never_ocean_lake_seen', '65_house_erie_on_lake',\n",
       "       '6_snow_winter_lake_cold', '24_level_water_sea_the',\n",
       "       '62_plants_flowers_native_foliage', '18_buffalo_city_and_niagara',\n",
       "       '26_erie_lake_live_on', '25_south_canada_southern_ontario',\n",
       "       '29_perry_battle_war_british', '21_fire_caught_cuyahoga_river',\n",
       "       '27_ytmdl_deepjyoti30_github_packages', '36_was_it_saw_the',\n",
       "       '52_lake_ontario_huron_canada', '48_michigan_the_border_of',\n",
       "       '13_monster_monsters_name_cleveland',\n",
       "       '56_snakes_snake_watersnake_water',\n",
       "       '41_balloons_event_balloonfest_guard',\n",
       "       '60_canadians_across_border_canada', '39_buffalo_ny_york_new',\n",
       "       '61_ocean_beach_was_it', '59_algae_blooms_toxic_bloom',\n",
       "       '33_mayflies_they_bugs_mayfly', '32_cedar_point_coaster_erie',\n",
       "       '64_sunsets_sunset_erie_lake', '22_my_was_she_me',\n",
       "       '46_november_came_gales_her', '63_live_erie_lake_from',\n",
       "       '16_swimming_swim_erie_lake', '42_erie_lake_gt_jay',\n",
       "       '34_ride_coaster_cedar_millennium', '37_beautiful_erie_lake_love',\n",
       "       '49_wind_storm_winds_erie', '44_war_british_battle_american',\n",
       "       '31_wind_power_nuclear_energy', '43_railroad_rail_train_line',\n",
       "       '58_he_her_was_she', '57_salt_mine_under_mines',\n",
       "       '30_boat_erie_lake_on', '51_shallow_deep_lake_is',\n",
       "       '40_district_trump_republican_county',\n",
       "       '53_canal_river_lawrence_the',\n",
       "       '54_university_medicine_college_school',\n",
       "       '19_20_number_action_play', '28_20_7d_message_20the',\n",
       "       '23_bros_erie_lake_lets', '50_20_henderson_number_action',\n",
       "       '38_20_number_action_play'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_document_info(texts).Name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "91e0f8c9-d124-419e-800c-43852870db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topics = topic_model.reduce_outliers(texts, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d94e838a-02e9-48a7-8e66-02d9e2f06647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50657"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "12abc853-6549-4080-8bf8-a29b7c87edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "count = Counter(new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7a2a1d63-72eb-4a28-88b2-4ddba62e8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, obj in enumerate(json_objects):\n",
    "    obj['topic_1'] = new_topics[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2e1d91fe-5e2b-4c4c-b63e-31dfbf3bd3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c219cb34-20bc-4927-b669-32364e6b1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topics = np.array(new_topics, dtype=np.int64).astype(np.int32)\n",
    "\n",
    "# Converting back to list for easy viewing/use in standard Python environments\n",
    "new_topics = new_topics.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ff4c52ad-df29-4fed-ab85-d14b9b3e268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = 'RC-topic1.json'\n",
    "with open(final_path, 'w') as file:\n",
    "    json.dump(json_objects, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "56a7c895-32a1-4807-a5d5-c43af8815e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 5010,\n",
       "         1: 3239,\n",
       "         2: 2152,\n",
       "         4: 2030,\n",
       "         15: 1771,\n",
       "         5: 1638,\n",
       "         10: 1603,\n",
       "         3: 1533,\n",
       "         17: 1473,\n",
       "         7: 1468,\n",
       "         11: 1452,\n",
       "         42: 1395,\n",
       "         22: 1234,\n",
       "         6: 1210,\n",
       "         8: 1084,\n",
       "         9: 1030,\n",
       "         26: 913,\n",
       "         52: 883,\n",
       "         12: 792,\n",
       "         16: 772,\n",
       "         20: 740,\n",
       "         23: 724,\n",
       "         25: 717,\n",
       "         24: 678,\n",
       "         14: 646,\n",
       "         13: 640,\n",
       "         63: 601,\n",
       "         45: 574,\n",
       "         37: 565,\n",
       "         61: 537,\n",
       "         48: 534,\n",
       "         18: 522,\n",
       "         39: 520,\n",
       "         30: 494,\n",
       "         21: 475,\n",
       "         19: 474,\n",
       "         36: 468,\n",
       "         47: 424,\n",
       "         29: 423,\n",
       "         32: 420,\n",
       "         28: 403,\n",
       "         53: 380,\n",
       "         60: 356,\n",
       "         33: 347,\n",
       "         49: 344,\n",
       "         27: 315,\n",
       "         55: 314,\n",
       "         51: 308,\n",
       "         31: 305,\n",
       "         34: 285,\n",
       "         40: 269,\n",
       "         35: 264,\n",
       "         44: 247,\n",
       "         43: 246,\n",
       "         59: 239,\n",
       "         38: 235,\n",
       "         56: 231,\n",
       "         57: 229,\n",
       "         58: 225,\n",
       "         65: 220,\n",
       "         41: 210,\n",
       "         46: 198,\n",
       "         54: 187,\n",
       "         50: 162,\n",
       "         64: 141,\n",
       "         62: 139})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa5565c8-3bed-44d7-91d4-147d7885fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from bertopic.representation import OpenAI\n",
    "\n",
    "# Fine-tune topic representations with GPT\n",
    "client = openai.OpenAI(api_key=\"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\")\n",
    "representation_model = OpenAI(client, model=\"gpt-3.5-turbo\", chat=True)\n",
    "topic_model = BERTopic(representation_model=representation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17e5c6b6-54fd-42ff-b886-2bc96c5bec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5eda5852-d2d2-468c-bd43-4d4485c9880a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\elain\\\\.cache\\\\huggingface\\\\hub\\\\models--sentence-transformers--all-MiniLM-L6-v2\\\\snapshots\\\\e4ce9877abf3edfe10b0d82785e83bdcb973e22e\\\\1_Pooling\\\\config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m topics, probs \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\bertopic\\lib\\site-packages\\bertopic\\_bertopic.py:385\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[1;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding - Transforming documents to embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mselect_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_embeddings(documents\u001b[38;5;241m.\u001b[39mDocument\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m    388\u001b[0m                                           images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    389\u001b[0m                                           method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    390\u001b[0m                                           verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    391\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding - Completed \u001b[39m\u001b[38;5;130;01m\\u2713\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\bertopic\\lib\\site-packages\\bertopic\\backend\\_utils.py:120\u001b[0m, in \u001b[0;36mselect_backend\u001b[1;34m(embedding_model, language)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sentencetransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerBackend\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m language\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSentenceTransformerBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m language\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m languages \u001b[38;5;129;01mor\u001b[39;00m language \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilingual\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SentenceTransformerBackend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\bertopic\\lib\\site-packages\\bertopic\\backend\\_sentencetransformers.py:44\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.__init__\u001b[1;34m(self, embedding_model)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m embedding_model\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embedding_model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_model \u001b[38;5;241m=\u001b[39m embedding_model\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda\\envs\\bertopic\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:191\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token)\u001b[0m\n\u001b[0;32m    188\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(model_name_or_path, token, cache_folder\u001b[38;5;241m=\u001b[39mcache_folder, revision\u001b[38;5;241m=\u001b[39mrevision):\n\u001b[1;32m--> 191\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[0;32m    200\u001b[0m         model_name_or_path,\n\u001b[0;32m    201\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    204\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    205\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda\\envs\\bertopic\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1246\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1239\u001b[0m             module_path \u001b[38;5;241m=\u001b[39m load_dir_path(\n\u001b[0;32m   1240\u001b[0m                 model_name_or_path,\n\u001b[0;32m   1241\u001b[0m                 module_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m                 revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1245\u001b[0m             )\n\u001b[1;32m-> 1246\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1247\u001b[0m     modules[module_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m module\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m modules\n",
      "File \u001b[1;32m~\\anaconda\\envs\\bertopic\\lib\\site-packages\\sentence_transformers\\models\\Pooling.py:227\u001b[0m, in \u001b[0;36mPooling.load\u001b[1;34m(input_path)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(input_path):\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fIn:\n\u001b[0;32m    228\u001b[0m         config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fIn)\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Pooling(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\elain\\\\.cache\\\\huggingface\\\\hub\\\\models--sentence-transformers--all-MiniLM-L6-v2\\\\snapshots\\\\e4ce9877abf3edfe10b0d82785e83bdcb973e22e\\\\1_Pooling\\\\config.json'"
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0a1db-dc40-43da-99ae-b2c5034fc1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
